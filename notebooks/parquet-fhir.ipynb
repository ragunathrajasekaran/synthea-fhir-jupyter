{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: fhir.resources in /opt/conda/lib/python3.11/site-packages (7.1.0)\n",
      "Requirement already satisfied: pydantic<3.0,>=2.0.1 in /opt/conda/lib/python3.11/site-packages (from pydantic[email]<3.0,>=2.0.1->fhir.resources) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0,>=2.0.1->pydantic[email]<3.0,>=2.0.1->fhir.resources) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0,>=2.0.1->pydantic[email]<3.0,>=2.0.1->fhir.resources) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0,>=2.0.1->pydantic[email]<3.0,>=2.0.1->fhir.resources) (4.8.0)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from pydantic[email]<3.0,>=2.0.1->fhir.resources) (2.2.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from email-validator>=2.0.0->pydantic[email]<3.0,>=2.0.1->fhir.resources) (2.6.1)\n",
      "Requirement already satisfied: idna>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from email-validator>=2.0.0->pydantic[email]<3.0,>=2.0.1->fhir.resources) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark\n",
    "%pip install fhir.resources\n",
    "\n",
    "# Directory containing Synthea-generated JSON files\n",
    "input_directory = \"/home/jovyan/work/fhir-data/fhir\"\n",
    "output_directory = \"/home/jovyan/work/fhir-data/fhir-parquet\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered resource types: Claim, Condition, DiagnosticReport, DocumentReference, Encounter, ExplanationOfBenefit, Immunization, Location, Organization, Patient, Practitioner, PractitionerRole, Provenance\n",
      "Resource counts before processing:\n",
      "claims: 344\n",
      "conditions: 33\n",
      "diagnosticreports: 344\n",
      "documentreferences: 344\n",
      "encounters: 344\n",
      "explanationofbenefits: 344\n",
      "immunizations: 472\n",
      "locations: 60\n",
      "organizations: 59\n",
      "patients: 33\n",
      "practitionerroles: 59\n",
      "practitioners: 59\n",
      "provenances: 33\n",
      "\n",
      "Processing claims...\n",
      "Number of rows: 344\n",
      "DataFrame created successfully for claims\n",
      "Parquet file written successfully for claims\n",
      "\n",
      "Processing conditions...\n",
      "Number of rows: 33\n",
      "DataFrame created successfully for conditions\n",
      "Parquet file written successfully for conditions\n",
      "\n",
      "Processing diagnosticreports...\n",
      "Number of rows: 344\n",
      "DataFrame created successfully for diagnosticreports\n",
      "Parquet file written successfully for diagnosticreports\n",
      "\n",
      "Processing documentreferences...\n",
      "Number of rows: 344\n",
      "DataFrame created successfully for documentreferences\n",
      "Parquet file written successfully for documentreferences\n",
      "\n",
      "Processing encounters...\n",
      "Number of rows: 344\n",
      "DataFrame created successfully for encounters\n",
      "Parquet file written successfully for encounters\n",
      "\n",
      "Processing explanationofbenefits...\n",
      "Number of rows: 344\n",
      "DataFrame created successfully for explanationofbenefits\n",
      "Parquet file written successfully for explanationofbenefits\n",
      "\n",
      "Processing immunizations...\n",
      "Number of rows: 472\n",
      "DataFrame created successfully for immunizations\n",
      "Parquet file written successfully for immunizations\n",
      "\n",
      "Processing locations...\n",
      "Number of rows: 60\n",
      "DataFrame created successfully for locations\n",
      "Parquet file written successfully for locations\n",
      "\n",
      "Processing organizations...\n",
      "Number of rows: 59\n",
      "DataFrame created successfully for organizations\n",
      "Parquet file written successfully for organizations\n",
      "\n",
      "Processing patients...\n",
      "Number of rows: 33\n",
      "DataFrame created successfully for patients\n",
      "Parquet file written successfully for patients\n",
      "\n",
      "Processing practitionerroles...\n",
      "Number of rows: 59\n",
      "DataFrame created successfully for practitionerroles\n",
      "Parquet file written successfully for practitionerroles\n",
      "\n",
      "Processing practitioners...\n",
      "Number of rows: 59\n",
      "DataFrame created successfully for practitioners\n",
      "Parquet file written successfully for practitioners\n",
      "\n",
      "Processing provenances...\n",
      "Number of rows: 33\n",
      "DataFrame created successfully for provenances\n",
      "Parquet file written successfully for provenances\n",
      "\n",
      "Resource counts after processing:\n",
      "claims: 344\n",
      "conditions: 33\n",
      "diagnosticreports: 344\n",
      "documentreferences: 344\n",
      "encounters: 344\n",
      "explanationofbenefits: 344\n",
      "immunizations: 472\n",
      "locations: 60\n",
      "organizations: 59\n",
      "patients: 33\n",
      "practitionerroles: 59\n",
      "practitioners: 59\n",
      "provenances: 33\n",
      "\n",
      "Resources that encountered errors:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, Any, List, Set\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from collections import Counter\n",
    "import traceback\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.appName(\"FHIR to Parquet\").getOrCreate()\n",
    "\n",
    "# Configuration\n",
    "INPUT_DIRECTORY = \"/home/jovyan/work/fhir-data/fhir\"\n",
    "OUTPUT_DIRECTORY = \"/home/jovyan/work/fhir-data/fhir-parquet\"\n",
    "\n",
    "def discover_resource_types(directory: str) -> Set[str]:\n",
    "    \"\"\"Discover all FHIR resource types in the input directory.\"\"\"\n",
    "    resource_types = set()\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                    entries = data.get('entry', [])\n",
    "                    for entry in entries:\n",
    "                        resource_type = entry['resource']['resourceType']\n",
    "                        resource_types.add(resource_type)\n",
    "    return resource_types\n",
    "\n",
    "def parse_fhir_resource(resource_data: Dict[str, Any]) -> Row:\n",
    "    \"\"\"Parse a FHIR resource into a PySpark Row.\"\"\"\n",
    "    # Convert the entire resource to a JSON string\n",
    "    return Row(resource=json.dumps(resource_data))\n",
    "\n",
    "def parse_fhir_bundle(file_path: str, resource_types: Set[str]) -> Dict[str, List[Row]]:\n",
    "    \"\"\"Parse a FHIR bundle file and return resources grouped by type.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        entries = data.get('entry', [])\n",
    "        \n",
    "        resources = {resource_type.lower() + \"s\": [] for resource_type in resource_types}\n",
    "        \n",
    "        for entry in entries:\n",
    "            resource = parse_fhir_resource(entry['resource'])\n",
    "            resource_type = entry['resource']['resourceType'].lower() + \"s\"\n",
    "            if resource_type in resources:\n",
    "                resources[resource_type].append(resource)\n",
    "        \n",
    "        return resources\n",
    "\n",
    "def process_fhir_files(resource_types: Set[str]) -> Dict[str, List[Row]]:\n",
    "    \"\"\"Process all FHIR files in the input directory.\"\"\"\n",
    "    all_resources = {resource_type.lower() + \"s\": [] for resource_type in resource_types}\n",
    "    resource_counts = Counter()\n",
    "    \n",
    "    for root, _, files in os.walk(INPUT_DIRECTORY):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                resources = parse_fhir_bundle(file_path, resource_types)\n",
    "                for resource_type, rows in resources.items():\n",
    "                    all_resources[resource_type].extend(rows)\n",
    "                    resource_counts[resource_type] += len(rows)\n",
    "    \n",
    "    print(\"Resource counts before processing:\")\n",
    "    for resource_type, count in sorted(resource_counts.items()):\n",
    "        print(f\"{resource_type}: {count}\")\n",
    "    \n",
    "    return all_resources\n",
    "\n",
    "def save_to_parquet(resources: Dict[str, List[Row]]):\n",
    "    \"\"\"Save resources to Parquet files.\"\"\"\n",
    "    processed_counts = Counter()\n",
    "    error_counts = Counter()\n",
    "    \n",
    "    # Define schema for all resource types\n",
    "    schema = StructType([\n",
    "        StructField(\"resource\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    for resource_type, rows in sorted(resources.items()):\n",
    "        if rows:\n",
    "            print(f\"\\nProcessing {resource_type}...\")\n",
    "            print(f\"Number of rows: {len(rows)}\")\n",
    "            \n",
    "            try:\n",
    "                df = spark.createDataFrame(rows, schema)\n",
    "                print(f\"DataFrame created successfully for {resource_type}\")\n",
    "                # df.printSchema()\n",
    "                # df.show(5, truncate=False)\n",
    "                \n",
    "                output_path = os.path.join(OUTPUT_DIRECTORY, resource_type)\n",
    "                df.write.mode('overwrite').parquet(output_path)\n",
    "                print(f\"Parquet file written successfully for {resource_type}\")\n",
    "                processed_counts[resource_type] = df.count()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {resource_type}: {e}\")\n",
    "                error_counts[resource_type] += 1\n",
    "                print(traceback.format_exc())\n",
    "    \n",
    "    print(\"\\nResource counts after processing:\")\n",
    "    for resource_type, count in sorted(processed_counts.items()):\n",
    "        print(f\"{resource_type}: {count}\")\n",
    "    \n",
    "    print(\"\\nResources that encountered errors:\")\n",
    "    for resource_type, count in sorted(error_counts.items()):\n",
    "        print(f\"{resource_type}: {count}\")\n",
    "\n",
    "# Main execution\n",
    "discovered_resource_types = discover_resource_types(INPUT_DIRECTORY)\n",
    "print(\"Discovered resource types:\", \", \".join(sorted(discovered_resource_types)))\n",
    "\n",
    "all_resources = process_fhir_files(discovered_resource_types)\n",
    "save_to_parquet(all_resources)\n",
    "\n",
    "# Don't stop the SparkSession here, as it might be needed for further operations in the notebook\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PySpark)",
   "language": "python",
   "name": "python3_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
