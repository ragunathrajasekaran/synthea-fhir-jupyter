{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.0)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: py4j\n",
      "Successfully installed py4j-0.10.9.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting fhir.resources\n",
      "  Downloading fhir.resources-7.1.0-py2.py3-none-any.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m202.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3.0,>=2.0.1 (from pydantic[email]<3.0,>=2.0.1->fhir.resources)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m580.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3.0,>=2.0.1->pydantic[email]<3.0,>=2.0.1->fhir.resources)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<3.0,>=2.0.1->pydantic[email]<3.0,>=2.0.1->fhir.resources)\n",
      "  Downloading pydantic_core-2.20.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0,>=2.0.1->pydantic[email]<3.0,>=2.0.1->fhir.resources) (4.8.0)\n",
      "Collecting email-validator>=2.0.0 (from pydantic[email]<3.0,>=2.0.1->fhir.resources)\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->pydantic[email]<3.0,>=2.0.1->fhir.resources)\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: idna>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from email-validator>=2.0.0->pydantic[email]<3.0,>=2.0.1->fhir.resources) (3.4)\n",
      "Downloading fhir.resources-7.1.0-py2.py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m912.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.20.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydantic-core, dnspython, annotated-types, pydantic, email-validator, fhir.resources\n",
      "Successfully installed annotated-types-0.7.0 dnspython-2.6.1 email-validator-2.2.0 fhir.resources-7.1.0 pydantic-2.8.2 pydantic-core-2.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark\n",
    "%pip install fhir.resources\n",
    "\n",
    "# Directory containing Synthea-generated JSON files\n",
    "input_directory = \"/home/jovyan/work/fhir-data/fhir\"\n",
    "output_directory = \"/home/jovyan/work/fhir-data/fhir-parquet\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered resource types: Claim, DiagnosticReport, DocumentReference, Encounter, ExplanationOfBenefit, Immunization, Location, Organization, Patient, Practitioner, PractitionerRole, Provenance\n",
      "Resource counts before processing:\n",
      "claims: 893\n",
      "diagnosticreports: 893\n",
      "documentreferences: 893\n",
      "encounters: 893\n",
      "explanationofbenefits: 893\n",
      "immunizations: 1292\n",
      "locations: 139\n",
      "organizations: 138\n",
      "patients: 100\n",
      "practitionerroles: 138\n",
      "practitioners: 138\n",
      "provenances: 100\n",
      "\n",
      "Processing claims...\n",
      "Number of rows: 893\n",
      "DataFrame created successfully for claims\n",
      "Parquet file written successfully for claims\n",
      "\n",
      "Processing diagnosticreports...\n",
      "Number of rows: 893\n",
      "DataFrame created successfully for diagnosticreports\n",
      "Parquet file written successfully for diagnosticreports\n",
      "\n",
      "Processing documentreferences...\n",
      "Number of rows: 893\n",
      "DataFrame created successfully for documentreferences\n",
      "Parquet file written successfully for documentreferences\n",
      "\n",
      "Processing encounters...\n",
      "Number of rows: 893\n",
      "DataFrame created successfully for encounters\n",
      "Parquet file written successfully for encounters\n",
      "\n",
      "Processing explanationofbenefits...\n",
      "Number of rows: 893\n",
      "DataFrame created successfully for explanationofbenefits\n",
      "Parquet file written successfully for explanationofbenefits\n",
      "\n",
      "Processing immunizations...\n",
      "Number of rows: 1292\n",
      "DataFrame created successfully for immunizations\n",
      "Parquet file written successfully for immunizations\n",
      "\n",
      "Processing locations...\n",
      "Number of rows: 139\n",
      "DataFrame created successfully for locations\n",
      "Parquet file written successfully for locations\n",
      "\n",
      "Processing organizations...\n",
      "Number of rows: 138\n",
      "DataFrame created successfully for organizations\n",
      "Parquet file written successfully for organizations\n",
      "\n",
      "Processing patients...\n",
      "Number of rows: 100\n",
      "DataFrame created successfully for patients\n",
      "Parquet file written successfully for patients\n",
      "\n",
      "Processing practitionerroles...\n",
      "Number of rows: 138\n",
      "DataFrame created successfully for practitionerroles\n",
      "Parquet file written successfully for practitionerroles\n",
      "\n",
      "Processing practitioners...\n",
      "Number of rows: 138\n",
      "DataFrame created successfully for practitioners\n",
      "Parquet file written successfully for practitioners\n",
      "\n",
      "Processing provenances...\n",
      "Number of rows: 100\n",
      "DataFrame created successfully for provenances\n",
      "Parquet file written successfully for provenances\n",
      "\n",
      "Resource counts after processing:\n",
      "claims: 893\n",
      "diagnosticreports: 893\n",
      "documentreferences: 893\n",
      "encounters: 893\n",
      "explanationofbenefits: 893\n",
      "immunizations: 1292\n",
      "locations: 139\n",
      "organizations: 138\n",
      "patients: 100\n",
      "practitionerroles: 138\n",
      "practitioners: 138\n",
      "provenances: 100\n",
      "\n",
      "Resources that encountered errors:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, Any, List, Set\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from collections import Counter\n",
    "import traceback\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.appName(\"FHIR to Parquet\").getOrCreate()\n",
    "\n",
    "# Configuration\n",
    "INPUT_DIRECTORY = \"/home/jovyan/work/fhir-data/fhir\"\n",
    "OUTPUT_DIRECTORY = \"/home/jovyan/work/fhir-data/fhir-parquet\"\n",
    "\n",
    "def discover_resource_types(directory: str) -> Set[str]:\n",
    "    \"\"\"Discover all FHIR resource types in the input directory.\"\"\"\n",
    "    resource_types = set()\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                    entries = data.get('entry', [])\n",
    "                    for entry in entries:\n",
    "                        resource_type = entry['resource']['resourceType']\n",
    "                        resource_types.add(resource_type)\n",
    "    return resource_types\n",
    "\n",
    "def parse_fhir_resource(resource_data: Dict[str, Any]) -> Row:\n",
    "    \"\"\"Parse a FHIR resource into a PySpark Row.\"\"\"\n",
    "    # Convert the entire resource to a JSON string\n",
    "    return Row(resource=json.dumps(resource_data))\n",
    "\n",
    "def parse_fhir_bundle(file_path: str, resource_types: Set[str]) -> Dict[str, List[Row]]:\n",
    "    \"\"\"Parse a FHIR bundle file and return resources grouped by type.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        entries = data.get('entry', [])\n",
    "        \n",
    "        resources = {resource_type.lower() + \"s\": [] for resource_type in resource_types}\n",
    "        \n",
    "        for entry in entries:\n",
    "            resource = parse_fhir_resource(entry['resource'])\n",
    "            resource_type = entry['resource']['resourceType'].lower() + \"s\"\n",
    "            if resource_type in resources:\n",
    "                resources[resource_type].append(resource)\n",
    "        \n",
    "        return resources\n",
    "\n",
    "def process_fhir_files(resource_types: Set[str]) -> Dict[str, List[Row]]:\n",
    "    \"\"\"Process all FHIR files in the input directory.\"\"\"\n",
    "    all_resources = {resource_type.lower() + \"s\": [] for resource_type in resource_types}\n",
    "    resource_counts = Counter()\n",
    "    \n",
    "    for root, _, files in os.walk(INPUT_DIRECTORY):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                resources = parse_fhir_bundle(file_path, resource_types)\n",
    "                for resource_type, rows in resources.items():\n",
    "                    all_resources[resource_type].extend(rows)\n",
    "                    resource_counts[resource_type] += len(rows)\n",
    "    \n",
    "    print(\"Resource counts before processing:\")\n",
    "    for resource_type, count in sorted(resource_counts.items()):\n",
    "        print(f\"{resource_type}: {count}\")\n",
    "    \n",
    "    return all_resources\n",
    "\n",
    "def save_to_parquet(resources: Dict[str, List[Row]]):\n",
    "    \"\"\"Save resources to Parquet files.\"\"\"\n",
    "    processed_counts = Counter()\n",
    "    error_counts = Counter()\n",
    "    \n",
    "    # Define schema for all resource types\n",
    "    schema = StructType([\n",
    "        StructField(\"resource\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    for resource_type, rows in sorted(resources.items()):\n",
    "        if rows:\n",
    "            print(f\"\\nProcessing {resource_type}...\")\n",
    "            print(f\"Number of rows: {len(rows)}\")\n",
    "            \n",
    "            try:\n",
    "                df = spark.createDataFrame(rows, schema)\n",
    "                print(f\"DataFrame created successfully for {resource_type}\")\n",
    "                # df.printSchema()\n",
    "                # df.show(5, truncate=False)\n",
    "                \n",
    "                output_path = os.path.join(OUTPUT_DIRECTORY, resource_type)\n",
    "                df.write.mode('overwrite').parquet(output_path)\n",
    "                print(f\"Parquet file written successfully for {resource_type}\")\n",
    "                processed_counts[resource_type] = df.count()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {resource_type}: {e}\")\n",
    "                error_counts[resource_type] += 1\n",
    "                print(traceback.format_exc())\n",
    "    \n",
    "    print(\"\\nResource counts after processing:\")\n",
    "    for resource_type, count in sorted(processed_counts.items()):\n",
    "        print(f\"{resource_type}: {count}\")\n",
    "    \n",
    "    print(\"\\nResources that encountered errors:\")\n",
    "    for resource_type, count in sorted(error_counts.items()):\n",
    "        print(f\"{resource_type}: {count}\")\n",
    "\n",
    "# Main execution\n",
    "discovered_resource_types = discover_resource_types(INPUT_DIRECTORY)\n",
    "print(\"Discovered resource types:\", \", \".join(sorted(discovered_resource_types)))\n",
    "\n",
    "all_resources = process_fhir_files(discovered_resource_types)\n",
    "save_to_parquet(all_resources)\n",
    "\n",
    "# Don't stop the SparkSession here, as it might be needed for further operations in the notebook\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PySpark)",
   "language": "python",
   "name": "python3_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
